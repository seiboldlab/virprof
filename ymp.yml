# Projects run with the pipeline. Each has a single CSV
# defining ids, samples and fastq file locations.
# IMPORTANT: Files MUST have a column named "sample" for
# grouping by the pipeline later.
projects:
  covid:
    data: projects/covid.csv
  daps:
    data: projects/daps.csv
  enigma:
    data: projects/enigma.csv
  gala:
    data: projects/gala.csv
  hrvc15:
    data: projects/hrvc15.csv
  inspire:
    data: projects/inspire.csv
  sarp:
    data: projects/sarp.csv
  sunbeam:
    data: projects/sunbeam.csv

# Here the pipeline is defined in three parts:
# 1. Selection of non-human reads
# 2. Assembly, read mapping and counting
# 3. Blast based binning and annotation
pipelines:
  remove_human_reads:
    # Trims reads and removes human reads by mapping to
    # hg38 reference first with hisat2 and then with bowtie2.
    - trim_bbmapAQ10
    - ref_hg38
    - map_hisat2
    - extract_readsf12
    - map_bowtie2
    - extract_readsf12
  assemble_and_map:
    # Removes low-entropy reads, assembles into contigs
    # >200bp length and computes coverage.
    - dust_bbmapE60
    - group_sample
    - assemble_metaspades
    - format_bbmapL200
    - index_bowtie2
    - map_bowtie2
    - sort_bam
    - coverage_samtools
  blast_classify:
    # Removes human contigs with megablast, executes
    # BlastN search against NT and creates annotated
    # bins.
    - ref_hg38
    - annotate_blastE2MegaBest
    - vp_remove_matchedU200
    - ref_NT
    - annotate_blastE10Best
    - ref_NcbiTaxonomy
    - vp_classify
  result:
    # Runs the full pipeline.
    - remove_human_reads
    - assemble_and_map
    - blast_classify

# Local cluster configuration:
cluster:
  # Which cluster engine profile to use
  profile: lsf
  profiles:
    lsf:
      args:
        # Add parameter selecting the queue
        #queue: "-q foo24"
        queue: "-q normal"
    default:
      # Limit how many jobs we keep in the queue
      # (Note: This is jobs, not cores as the name would suggest)
      cluster_cores: 50
      # Limit the number of jobs submitted per second
      max_jobs_per_second: 5
      # Wait very long for files to appear. Our NFS can be
      # extremely slow to sync from node to node.
      latency_wait: 300
limits:
  # All jobs should request at least 2GB RAM.
  min_mem: 2G
  # Scale large jobs (mapping, assembly) down to the
  # max memory our regular compute nodes have available.
  max_mem: 45G

# Configure local references
references:
  # Human reference used for read and contig filtering:
  hg38:
    # The index files for Bowtie2
    - url:
        /data/reference/iGenomes/Homo_sapiens/UCSC/hg38/Sequence/Bowtie2Index/
      type: dirx
      files:
        ALL.1.bt2: genome.1.bt2
        ALL.2.bt2: genome.2.bt2
        ALL.3.bt2: genome.3.bt2
        ALL.4.bt2: genome.4.bt2
        ALL.rev.1.bt2: genome.rev.1.bt2
        ALL.rev.2.bt2: genome.rev.2.bt2
        ALL.fasta: genome.fa
    # The index files for Blast
    - url: /data/reference/iGenomes/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/
      type: dirx
      files:
        ALL.nin: genome_blastn.nin
        ALL.nhr: genome_blastn.nhr
        ALL.nsq: genome_blastn.nsq
        ALL.nsi: genome_blastn.nsi
        ALL.nsd: genome_blastn.nsd
        ALL.nog: genome_blastn.nog
    # The index files for Hisat2
    - url: /Seibold/data/Reference/HISAT/grch38_snp_tran/
      type: dirx
      files:
        ALL.1.ht2: genome_snp_tran.1.ht2
        ALL.2.ht2: genome_snp_tran.2.ht2
        ALL.3.ht2: genome_snp_tran.3.ht2
        ALL.4.ht2: genome_snp_tran.4.ht2
        ALL.5.ht2: genome_snp_tran.5.ht2
        ALL.6.ht2: genome_snp_tran.6.ht2
        ALL.7.ht2: genome_snp_tran.7.ht2
        ALL.8.ht2: genome_snp_tran.8.ht2
  # NCBI Nucleotide database for Blast
  NT:
    - url: /data/reference/NCBI_nt/NCBI_NT_2020-05-01/
      type: path
      group:
        - nt
      match:
        - (?P<sample>[^.]+)\.((nal|not|nto|ntf|nos|ndb)|[0-9]+\.(nin|nhr|nsq|nsd|nog))
  # NCBI Taxonomy dump for classification
  NcbiTaxonomy:
    - url: /data/reference/NCBI/taxonomy/
      type: dirx
      files:
        ALL.NCBI.nodes.dmp: nodes.dmp
        ALL.NCBI.names.dmp: names.dmp
  # SILVA databases for removing rRNA (unused at this time)
  SilvaSSU:
    - url: https://www.arb-silva.de/fileadmin/silva_databases/release_138/Exports/SILVA_138_SSURef_NR99_tax_silva_trunc.fasta.gz
  SilvaLSU:
    - url: https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_LSURef_tax_silva_trunc.fasta.gz
