# Projects run with the pipeline. Each has a single CSV
# defining units, samples and fastq file locations.
projects:
  covid:
    data: projects/covid.csv
  copd:
    data: /Seibold/data2/COPDGene_Unmapped/mapping.csv
  daps:
    data: projects/daps.csv
  enigma:
    data: projects/enigma.csv
  endotype:
    data: projects/endotype.csv
  gala:
    data: projects/gala.csv
  hrvc15:
    data: projects/hrvc15.csv
  inspire:
    data: projects/inspire.csv
  lemieux:
    data: projects/lemieux.csv
  primero:
    data: projects/primero.csv
  sarp:
    data: projects/sarp.csv
  sunbeam:
    data: projects/sunbeam.csv
  test:
    data: projects/test.csv

pipelines:
  ### Sub-Pipelines ###
  # Trimming and FastQC
  qc:
    stages:
      - qc_fastqc
      - trim_bbmapAQ10L70
      - qc_fastqc

  # Keep *only* reads matching last *.fasta.gz (from ref)
  enrich:
    stages:
      - map_bbmapS:
          hide: true
      - extract_readsG12

  # Remove reads matching last bowtie2 and hisat2 indicies (from ref)
  deplete:
    stages:
      - map_hisat2:
          hide: true
      - extract_readsf12:
          hide: true
      - map_bowtie2:
          hide: true
      - extract_readsf12

  # Assemble (by sample) using Spades Single Cell mode and remove short contigs
  assemble_single:
    stages:
      - group_sample
      - assemble_spadesSc:
          hide: true
      - format_bbmapL200

  # Assemble (by sample) using Spades Metagenomic mode and remove short contigs
  # (removes low entropy reads prior to assembly)
  assemble_meta:
    stages:
      - dust_bbmapE60:
          hide: true
      - group_sample
      - assemble_spadesMeta:
          hide: true
      - format_bbmapL200

  # Compute coverage and sorted bam file using bowtie2
  coverage:
    stages:
      - index_bowtie2:
          hide: true
      - map_bowtie2S:
          hide: true
      - sort_bam
      - coverage_samtools

  # Remove human contigs and create classified bins using BLAST
  blastbin:
    stages:
      - ref_hg38:
          hide: true
      - annotate_blastE2MegaBest:
          hide: true
      - blastfilter_vpU200:
          hide: true
      - ref_NT:
          hide: true
      - annotate_blastE10Best:
          hide: true
      - ref_NcbiTaxonomy:
          hide: true
      - group_sample:
          hide: true
      - blastbin_vp

  # Re-Assemble created bins
  reassemble:
    stages:
      - enrich:
          hide: true
      - assemble_spadesSc:
          hide: true
      - group_sample
      - format_bbmapL200

  # Reference guidedworkflow
  targeted:
    stages:
      - qc
      - enrich
      - assemble_single
      - coverage
      - blastbin

  generic:
    stages:
      - qc
      - ref_hg38
      - deplete
      - assemble_meta
      - coverage
      - blastbin
      - reassemble
      - coverage
      - blastbin

# Local cluster configuration:
cluster:
  # Which cluster engine profile to use
  profile: slurm
  profiles:
    slurm:
      args:
        # Add parameter selecting the queue
        queue: "-p normal"
        comment: "--comment {output[0]}"
    lsf:
      args:
        # Add parameter selecting the queue
        queue: "-q foo24"
    default:
      # Limit how many jobs we keep in the queue
      # (Note: This is jobs, not cores as the name would suggest)
      cluster_cores: 500
      # Limit the number of jobs submitted per second
      max_jobs_per_second: 5
      # Wait very long for files to appear. Our NFS can be
      # extremely slow to sync from node to node.
      latency_wait: 600


limits:
  # All jobs should request at least 8GB RAM.
  min_mem: 8G
  # Scale large jobs (mapping, assembly) down to the
  # max memory our regular compute nodes have available.
  max_mem: 256G


# Configure local references
references:
  # Human reference used for read and contig filtering:
  hg38:
    # The index files for Bowtie2
    - url:
        /data/reference/iGenomes/Homo_sapiens/UCSC/hg38/Sequence/Bowtie2Index/
      type: dirx
      files:
        ALL.1.bt2: genome.1.bt2
        ALL.2.bt2: genome.2.bt2
        ALL.3.bt2: genome.3.bt2
        ALL.4.bt2: genome.4.bt2
        ALL.rev.1.bt2: genome.rev.1.bt2
        ALL.rev.2.bt2: genome.rev.2.bt2
        ALL.fasta: genome.fa
    # The index files for Blast
    - url: /data/reference/iGenomes/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/
      type: dirx
      files:
        ALL.nin: genome_blastn.nin
        ALL.nhr: genome_blastn.nhr
        ALL.nsq: genome_blastn.nsq
        ALL.nsi: genome_blastn.nsi
        ALL.nsd: genome_blastn.nsd
        ALL.nog: genome_blastn.nog
    # The index files for Hisat2
    - url: /Seibold/data/Reference/HISAT/grch38_snp_tran/
      type: dirx
      files:
        ALL.1.ht2: genome_snp_tran.1.ht2
        ALL.2.ht2: genome_snp_tran.2.ht2
        ALL.3.ht2: genome_snp_tran.3.ht2
        ALL.4.ht2: genome_snp_tran.4.ht2
        ALL.5.ht2: genome_snp_tran.5.ht2
        ALL.6.ht2: genome_snp_tran.6.ht2
        ALL.7.ht2: genome_snp_tran.7.ht2
        ALL.8.ht2: genome_snp_tran.8.ht2
  # NCBI Nucleotide database for Blast
  NT:
    - url: /data/reference/NCBI_nt/NCBI_NT_2020-05-01/
      type: path
      id: nt
      match:
        - (?P<sample>[^.]+)\.((nal|not|nto|ntf|nos|ndb)|[0-9]+\.(nin|nhr|nsq|nsd|nog))
  # NCBI Taxonomy dump for classification
  NcbiTaxonomy:
    - url: /data/reference/NCBI/taxonomy/
      type: dirx
      files:
        ALL.NCBI.nodes.dmp: nodes.dmp
        ALL.NCBI.names.dmp: names.dmp
  # SILVA databases for removing rRNA (unused at this time)
  SilvaSSU:
    - url: https://www.arb-silva.de/fileadmin/silva_databases/release_138/Exports/SILVA_138_SSURef_NR99_tax_silva_trunc.fasta.gz
  SilvaLSU:
    - url: https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_LSURef_tax_silva_trunc.fasta.gz
  hrvc:
    - url: data/hrv_c_15.fasta.gz
  scov2:
    - url: data/sars_cov_2.fasta.gz
  hrvpb:
    - url: data/hrv_pb_unaligned.fasta.gz
